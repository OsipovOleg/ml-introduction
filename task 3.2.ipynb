{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"},"source":["<center>\n","<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n","    \n","## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n","Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."]},{"cell_type":"markdown","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"},"source":["## <center> Assignment 4. Sarcasm detection with logistic regression\n","    \n","We'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n","\n","Sarcasm detection is easy. \n","<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247","execution":{"iopub.execute_input":"2021-12-15T11:13:10.418358Z","iopub.status.busy":"2021-12-15T11:13:10.417776Z","iopub.status.idle":"2021-12-15T11:13:11.198730Z","shell.execute_reply":"2021-12-15T11:13:11.197737Z","shell.execute_reply.started":"2021-12-15T11:13:10.418307Z"},"trusted":true},"outputs":[],"source":["!ls ../input/sarcasm/"]},{"cell_type":"code","execution_count":3,"metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.execute_input":"2021-12-15T11:13:11.200455Z","iopub.status.busy":"2021-12-15T11:13:11.200173Z","iopub.status.idle":"2021-12-15T11:13:12.661481Z","shell.execute_reply":"2021-12-15T11:13:12.660662Z","shell.execute_reply.started":"2021-12-15T11:13:11.200408Z"},"trusted":true},"outputs":[],"source":["# some necessary imports\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import seaborn as sns\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.execute_input":"2021-12-15T11:13:12.663057Z","iopub.status.busy":"2021-12-15T11:13:12.662805Z","iopub.status.idle":"2021-12-15T11:13:20.918436Z","shell.execute_reply":"2021-12-15T11:13:20.917315Z","shell.execute_reply.started":"2021-12-15T11:13:12.663016Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:13:20.920321Z","iopub.status.busy":"2021-12-15T11:13:20.919937Z","iopub.status.idle":"2021-12-15T11:13:21.085188Z","shell.execute_reply":"2021-12-15T11:13:21.084424Z","shell.execute_reply.started":"2021-12-15T11:13:20.920249Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.execute_input":"2021-12-15T11:13:21.086733Z","iopub.status.busy":"2021-12-15T11:13:21.086459Z","iopub.status.idle":"2021-12-15T11:13:21.116093Z","shell.execute_reply":"2021-12-15T11:13:21.115122Z","shell.execute_reply.started":"2021-12-15T11:13:21.086681Z"},"trusted":true},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.execute_input":"2021-12-15T11:13:21.117646Z","iopub.status.busy":"2021-12-15T11:13:21.117362Z","iopub.status.idle":"2021-12-15T11:13:21.908331Z","shell.execute_reply":"2021-12-15T11:13:21.907643Z","shell.execute_reply.started":"2021-12-15T11:13:21.117596Z"},"trusted":true},"outputs":[],"source":["train_df.info()"]},{"cell_type":"markdown","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"},"source":["Some comments are missing, so we drop the corresponding rows."]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.execute_input":"2021-12-15T11:13:21.909862Z","iopub.status.busy":"2021-12-15T11:13:21.909442Z","iopub.status.idle":"2021-12-15T11:13:22.220404Z","shell.execute_reply":"2021-12-15T11:13:22.219531Z","shell.execute_reply.started":"2021-12-15T11:13:21.909803Z"},"trusted":true},"outputs":[],"source":["train_df.dropna(subset=['comment'], inplace=True)"]},{"cell_type":"markdown","metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"},"source":["We notice that the dataset is indeed balanced"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11","execution":{"iopub.execute_input":"2021-12-15T11:13:22.222181Z","iopub.status.busy":"2021-12-15T11:13:22.221848Z","iopub.status.idle":"2021-12-15T11:13:22.242583Z","shell.execute_reply":"2021-12-15T11:13:22.241533Z","shell.execute_reply.started":"2021-12-15T11:13:22.222116Z"},"trusted":true},"outputs":[],"source":["train_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"},"source":["We split data into training and validation parts."]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96","execution":{"iopub.execute_input":"2021-12-15T11:13:22.244620Z","iopub.status.busy":"2021-12-15T11:13:22.244059Z","iopub.status.idle":"2021-12-15T11:13:22.516223Z","shell.execute_reply":"2021-12-15T11:13:22.515498Z","shell.execute_reply.started":"2021-12-15T11:13:22.244336Z"},"trusted":true},"outputs":[],"source":["train_texts, valid_texts, y_train, y_valid = \\\n","        train_test_split(train_df['comment'], train_df['label'], random_state=17)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:13:22.517870Z","iopub.status.busy":"2021-12-15T11:13:22.517643Z","iopub.status.idle":"2021-12-15T11:13:23.605715Z","shell.execute_reply":"2021-12-15T11:13:23.604539Z","shell.execute_reply.started":"2021-12-15T11:13:22.517830Z"},"trusted":true},"outputs":[],"source":["train_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).hist(label='sarcastic')\n","train_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).hist(label='normal')\n","plt.legend();"]},{"cell_type":"markdown","metadata":{},"source":["где более саркастичнны"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:16:38.880681Z","iopub.status.busy":"2021-12-15T11:16:38.880008Z","iopub.status.idle":"2021-12-15T11:16:39.213552Z","shell.execute_reply":"2021-12-15T11:16:39.212571Z","shell.execute_reply.started":"2021-12-15T11:16:38.880611Z"},"trusted":true},"outputs":[],"source":["sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\n","sub_df.sort_values(by='sum', ascending=False).head(10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:16:51.599286Z","iopub.status.busy":"2021-12-15T11:16:51.598733Z","iopub.status.idle":"2021-12-15T11:16:51.617205Z","shell.execute_reply":"2021-12-15T11:16:51.616061Z","shell.execute_reply.started":"2021-12-15T11:16:51.599237Z"},"trusted":true},"outputs":[],"source":["sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)\n"]},{"cell_type":"markdown","metadata":{},"source":["комменты от одного автора ничего особо не дают"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:17:29.594871Z","iopub.status.busy":"2021-12-15T11:17:29.594527Z","iopub.status.idle":"2021-12-15T11:17:32.556087Z","shell.execute_reply":"2021-12-15T11:17:32.555091Z","shell.execute_reply.started":"2021-12-15T11:17:29.594804Z"},"trusted":true},"outputs":[],"source":["sub_df = train_df.groupby('author')['label'].agg([np.size, np.mean, np.sum])\n","sub_df[sub_df['size'] > 300].sort_values(by='mean', ascending=False).head(10)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:21:13.915416Z","iopub.status.busy":"2021-12-15T11:21:13.914950Z","iopub.status.idle":"2021-12-15T11:21:13.921422Z","shell.execute_reply":"2021-12-15T11:21:13.920790Z","shell.execute_reply.started":"2021-12-15T11:21:13.915351Z"},"trusted":true},"outputs":[],"source":["# установим максимальное число фичей и ограничим минимальную частоту слов, создаем биграмы.lbfgs - полиномиальные потери\n","tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n","# полиномиальная логистическая регрессия\n","logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n","                           random_state=17, verbose=1)\n","# конвейер skLearn\n","tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n","                                 ('logit', logit)])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-12-15T11:46:53.026629Z","iopub.status.busy":"2021-12-15T11:46:53.026193Z","iopub.status.idle":"2021-12-15T11:46:53.552125Z","shell.execute_reply":"2021-12-15T11:46:53.551285Z","shell.execute_reply.started":"2021-12-15T11:46:53.026572Z"},"trusted":true},"outputs":[],"source":["import eli5\n","eli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n","                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])"]},{"cell_type":"markdown","metadata":{"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"},"source":["## Tasks:\n","1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n","2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n","3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n","4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n","\n","## Links:\n","  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n","  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n","  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n","  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
